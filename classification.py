# -*- coding: utf-8 -*-
"""classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oDBFD4DKEZU4rPAMjZ2gFmbekKM0KyBl
"""

import pandas as pd
import numpy as np
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from collections import defaultdict
import torch
from torch.utils.data import Dataset, DataLoader
from nltk.tokenize import sent_tokenize, word_tokenize
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import random
import string
nltk.download('punkt')
nltk.download('stopwords')
import string
import os

class preprocess_data():
    def __init__(self, file_path):
        self.filepath = file_path
        self.word2idx = {}
        self.idx2word = {}
        self.vocab = []
        self.vocab_size = 0

        self.words = []
        self.labels = []
        self.remove_words()

    def preprocess(self , data , stop_words , ps):
        data = " ".join(data)
        data = data.lower()
        data1 = [char if char not in string.punctuation else ' ' for char in data]
        pre_data = "".join(data1)
        words = pre_data.split()
        cleaned_string = ' '.join(words)

        return cleaned_string

    def remove_words(self):
        data_df = pd.read_csv(self.filepath)
        data = data_df['Description']
        data = data.tolist()

        labels = data_df['Class Index']
        labels = labels.tolist()

        stop_words = set(stopwords.words('english'))
        ps = PorterStemmer()

        preprocessed_sentences= []
        labels_encoded = []
        for sentence, label in tqdm(zip(data, labels)):
            sentence = sent_tokenize(sentence)
            sentence = self.preprocess(sentence, stop_words, ps)
            preprocessed_sentences.append(sentence)
            labels_encoded.append(label)

        self.words = preprocessed_sentences
        self.labels = labels_encoded
        word_freq = defaultdict(int)
        preprocessed_sentences = preprocessed_sentences[:20000]
        word2idx = {}
        idx2word = {}
        word2idx['<PAD>'] = 0
        word2idx['<UNK>'] = 1
        idx2word[0] = '<PAD>'
        idx2word[1] = '<UNK>'
        vocab = []
        vocab.append('<PAD>')
        vocab.append('<UNK>')
        vocab_size = 2
        for sentence in preprocessed_sentences :
            sentence = sentence.split()
            for word in sentence :
                word_freq[word] += 1

        for (word , freq) in (word_freq.items()):
            word2idx[word] = vocab_size
            idx2word[vocab_size] = word
            vocab.append(word)
            vocab_size+=1

        self.word2idx = word2idx
        self.idx2word = idx2word
        self.vocab = vocab
        self.vocab_size = vocab_size


class CustomDataset(Dataset):
    def __init__(self, data, preprocess_data, max_len=None):
        self.preprocess_data = preprocess_data
        self.word2idx = self.preprocess_data.word2idx
        self.data = data
        self.max_len = max(len(word_tokenize(sent)) for sent in self.data['Description'])
        self.tokens = []
        self.labels = []
        for index, row in self.data.iterrows():
            sentence = row['Description']
            label = row['Class Index']
            tokens = self.padding(sentence)
            self.tokens.append(tokens)
            self.labels.append(label)

    def padding(self, sentence):
        words = word_tokenize(sentence)
        tokens = [self.word2idx[word] if word in self.word2idx else self.word2idx['<UNK>'] for word in words]
        if self.max_len is not None:
            tokens = tokens[:self.max_len] + [self.word2idx['<PAD>']] * (self.max_len - len(tokens))
        return tokens

    def __getitem__(self, index):
        tokens = self.tokens[index]
        label = self.labels[index]
        forward_data = tokens[1:]
        backward_data = tokens[:-1]
        return  torch.tensor(tokens), torch.tensor(label)

    def __len__(self):
        return len(self.data)



class elmo_model(nn.Module):
    def __init__(self, vocab_size, embedding, hidden_dim):
        super(elmo_model, self).__init__()
        self.vocab_size = vocab_size
        self.hidden_dim = hidden_dim
        self.embedding = embedding
        self.lstm1 = nn.LSTM(embedding.embedding_dim, hidden_dim,batch_first=True, bidirectional=True)
        self.lstm2 = nn.LSTM(hidden_dim*2, hidden_dim,batch_first=True, bidirectional=True)
        self.linear_out = nn.Linear(hidden_dim*2, vocab_size)

    def forward(self, back_data):
        back_embed = self.embedding(back_data)
        back_lstm1, _ = self.lstm1(back_embed)
        back_lstm2, _ = self.lstm2(back_lstm1)
        linear_out = self.linear_out(back_lstm2)
        return linear_out

dataset_model = preprocess_data('train.csv')

vocab = dataset_model.vocab
word2idx = dataset_model.word2idx
idx2word = dataset_model.idx2word
vocab_size = dataset_model.vocab_size

data1 = pd.read_csv('train.csv')
data1 = data1[:20000]

train_dataset = CustomDataset(data1 , dataset_model)

val_data = pd.read_csv('train.csv')
val_data = val_data[20001:25001]
valid_dataset = CustomDataset(val_data , dataset_model)

test_data = pd.read_csv('test.csv')
test_dataset = CustomDataset(test_data , dataset_model)

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Path to the GloVe file in your Google Drive
glove_file_path = '/content/drive/My Drive/glove.6B.300d.txt'

glove_dict = {}
with open(glove_file_path, 'r', encoding='utf-8') as f:
    for line in f:
        tokens = line.strip().split(' ')
        word = tokens[0]
        embedding = np.array([float(val) for val in tokens[1:]])
        glove_dict[word] = embedding

UNK_emb = np.mean(list(glove_dict.values()), axis=0)

PAD_emb = np.zeros(300)

vocab = dataset_model.vocab
embeddings = []
for word in vocab:
    if word == '<UNK>':
        embeddings.append(UNK_emb)
    elif word == '<PAD>':
        embeddings.append(PAD_emb)
    elif word in glove_dict:
        embeddings.append(glove_dict[word])
    else:
        emb = np.random.uniform(-0.25, 0.25, 300)
        embeddings.append(emb)

embeddings = torch.tensor(embeddings, dtype=torch.float)

embedding = nn.Embedding.from_pretrained(embeddings, freeze=False, padding_idx=0)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
VOCAB_SIZE = vocab_size
BATCH_SIZE = 64
EMBEDDING_DIM = 300
HIDDEN_DIM = 100

embedding = nn.Embedding.from_pretrained(embeddings, freeze=False, padding_idx=0)

elmo = elmo_model(VOCAB_SIZE, embedding, HIDDEN_DIM)
elmo.to(device)
optimizer = optim.Adam(elmo.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss(ignore_index=0)

model_file = 'bilstm.pt'
if(os.path.exists(model_file)):
    elmo.load_state_dict(torch.load(model_file))

elmo_embeddings = None
embedding_file = 'elmo_embeds.pt'
if(os.path.exists(embedding_file)):
    elmo_embeddings = torch.load(embedding_file)
    print("EMBEDDINGS LOADED SUCCESSFULLY")

elmo_lstm_layer1 = elmo.lstm1
print(elmo_lstm_layer1)

elmo_lstm_layer2 = elmo.lstm2
print(elmo_lstm_layer2)

elmo_embeddings = list(elmo.parameters())[0]
elmo_embeddings = elmo_embeddings.to(device)


train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False )
val_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE , shuffle = False)

class downstream_task(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, elmoEmbedding, elmo_lstm1, elmo_lstm2, num_classes , lambda_weights):
        super(downstream_task, self).__init__()

        self.embeddings = nn.Embedding.from_pretrained(elmoEmbedding)
        self.weights = nn.Parameter(torch.tensor(lambda_weights), requires_grad=False)
        self.lstm1 = elmo_lstm1
        self.lstm2 = elmo_lstm2
        self.linear1 = nn.Linear(embedding_dim, hidden_dim*2)
        self.linear_out = nn.Linear(hidden_dim*2, num_classes)
        self.dropout = nn.Dropout(0.5)

    def forward(self, input_data):
        embed = self.embeddings(input_data)
        embedChange = self.linear1(embed)
        lstm1, _ = self.lstm1(embed)
        lstm2, _ = self.lstm2(lstm1)

        lambda1 = self.weights[0]
        lambda2 = self.weights[1]
        lambda3 = self.weights[2]
        elmo_out = (lambda1*lstm1 + lambda2*lstm2 + lambda3*embedChange)/(lambda1+lambda2+lambda3)
        elmo_max = torch.max(elmo_out, dim=1)[0]
        elmo_max = self.dropout(elmo_max)
        linear_out = self.linear_out(elmo_max)
        return linear_out

td = pd.read_csv('./train.csv')
td_labels = td['Class Index']

train_labels = td_labels[:20000]
valid_labels = td_labels[20001:25001]

td1 = pd.read_csv('./test.csv')
test_labels = td1['Class Index']


input_size = vocab_size
hidden_dim = 100
embedding_dim = 300
output_size = len(set(train_labels))+1

lambda_weights = [0.3 , 0.3 , 0.3]

classifier = downstream_task(input_size, embedding_dim , hidden_dim  , elmo_embeddings ,elmo_lstm_layer1 , elmo_lstm_layer2 , output_size, lambda_weights)
optimizer = optim.Adam(classifier.parameters(), lr=0.001 , weight_decay= 0.0001)
criterion = nn.CrossEntropyLoss()
classifier.to(device)

from sklearn.metrics import f1_score, confusion_matrix, accuracy_score

def train_and_validate(model, train_dataloader, valid_dataloader, optimizer, criterion, device=device, epochs=5):
    model.to(device)
    train_losses = []
    valid_losses = []
    train_accuracies = []
    valid_accuracies = []

    for epoch in range(1, epochs + 1):
        print(f"\nEpoch {epoch}/{epochs}:")

        # Training
        model.train()
        train_loss = 0
        total_correct = 0
        total_samples = 0

        for batch in tqdm(train_dataloader, desc="Training"):
            optimizer.zero_grad()
            input_data, labels = batch
            input_data, labels = input_data.to(device), labels.to(device)

            logits  = model(input_data)
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

            _, preds = torch.max(logits, dim=1)
            total_correct += torch.sum(preds == labels).item()
            total_samples += labels.size(0)

        avg_train_loss = train_loss / len(train_dataloader)
        avg_train_accuracy = total_correct / total_samples
        train_losses.append(avg_train_loss)
        train_accuracies.append(avg_train_accuracy)
        print(f"  Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_accuracy:.4f}")

        # Validation
        model.eval()
        valid_loss = 0
        total_correct = 0
        total_samples = 0
        max_valid_acc = 0

        with torch.no_grad():
            for batch in tqdm(valid_dataloader, desc="Validation"):
                input_data, labels= batch
                input_data, labels = input_data.to(device), labels.to(device)

                logits = model(input_data)
                loss = criterion(logits, labels)

                valid_loss += loss.item()

                _, preds = torch.max(logits, dim=1)
                total_correct += torch.sum(preds == labels).item()
                total_samples += labels.size(0)

        avg_valid_loss = valid_loss / len(valid_dataloader)
        avg_valid_accuracy = total_correct / total_samples
        valid_losses.append(avg_valid_loss)
        valid_accuracies.append(avg_valid_accuracy)
        print(f"  Validation Loss: {avg_valid_loss:.4f}, Validation Accuracy: {avg_valid_accuracy:.4f}")

        # Check if the current validation accuracy is the best we've seen so far
        if avg_valid_accuracy > max_valid_acc:
            max_valid_acc = avg_valid_accuracy
            # Save model
            torch.save(model.state_dict(), f'best_{modelName}.pt')
            print(f"Saved new best model with Validation Accuracy")

    return train_losses, valid_losses, train_accuracies, valid_accuracies

train_losses, valid_losses, train_accuracies, valid_accuracies = train_and_validate(classifier , train_dataloader , val_dataloader , optimizer , criterion)

# Save classifier model
#torch.save(classifier.state_dict(), 'classifier.pt')

classifier.eval()
test_loss = 0
correct = 0
total = 0
with torch.no_grad():
    for batch in test_dataloader:
        input_data, labels = batch[0].to(device), batch[1].to(device)
        outputs = classifier(input_data)
        loss = criterion(outputs, labels)
        test_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

accuracy = 100 * correct / total
avg_loss = test_loss / len(test_dataloader)
print('\nTest Loss: {:.4f}, Accuracy: {:.2f}%'.format(avg_loss, accuracy))

import matplotlib.pyplot as plt

# Plotting Training and Validation Loss
plt.figure(figsize=(10, 5))
plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')
plt.plot(range(1, len(valid_losses) + 1), valid_losses, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix

f1 = f1_score(labels.cpu(), predicted.cpu(), average='weighted')
recall = recall_score(labels.cpu(), predicted.cpu(), average='weighted')
precision = precision_score(labels.cpu(), predicted.cpu(), average='weighted')
cm = confusion_matrix(labels.cpu(), predicted.cpu())

print(f'F1 Score: {f1:.4f}')
print(f'Recall: {recall:.4f}')
print(f'Precision: {precision:.4f}')
print('Confusion Matrix:')
print(cm)